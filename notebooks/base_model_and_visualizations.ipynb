{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\n",
    "\n",
    "*   Text classification \n",
    "*   Represent text in meaningful way\n",
    "    *   labelled inputs (supervised learning)\n",
    "    *   multi-classification model (unsupervised learning)\n",
    "*    Prediction based on classification patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different approaches to text mining: data mining, machine learning, information retrieval and knowledge management.  Each seeks to extract, identify and use information from large collections of textual data.  \n",
    "\n",
    "Text classification is a learning process of text mining.  In this use case it involves preprocessing the data,  weighting terms, using the KNN algorithm in combination with  the K-means clustering algorithim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: until we have data documents to process, this will serve only as an option for future teams - inherited DB from Labs29 is empty with no schema\n",
    "\n",
    "[Labs 29  HRF Asylum B DS AWS RDS PostgresSQL](https://master:***@asylum.catpmmwmrkhp.us-east-1.rds.amazonaws.com/asylum)\n",
    "\n",
    "[Labs 29 AWS Asylum A DS RDS PostgresSQL](https://master:***rds_endpoint=hrfasylum-database-a.catpmmwmrkhp.us-east-1.rds.amazonaws.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Model and  Visualizations \n",
    "\n",
    "The purpose of the model is to classify the legal documents containing judicial decisions for immigration assylum seekers:\n",
    "\n",
    "1.   Judicial decision\n",
    "\n",
    "    *   Asylum Granted\n",
    "    *   Asylum Relief Denied\n",
    "    *   Other Relief Granted\n",
    "    *   Admin Closure (expired)\n",
    "\n",
    "\n",
    "2.   Insights from patterns in data of individual judges (IJ cases only)\n",
    "3.   Insights from patterns in data of appeallate (panel) judges (BIA cases only)\n",
    "4.   Insights from patterns in all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database dependency\n",
    "!pip install sqlalchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dependency\n",
    "!pip install pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# dataframe tools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# natural language processor\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "# wrapper to show progress/time as self checking tool\n",
    "from tqdm import tqd\n",
    "\n",
    "\n",
    "# key word extraction\n",
    "import textacy\n",
    "import textacy.ke\n",
    "\n",
    "\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "\n",
    "# linear algebra and array/matrices tools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# database tools\n",
    "import sqlalchemy\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pydantic import BaseModel \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database URL\n",
    "database_url = input()\n",
    "\n",
    "#connect to database\n",
    "engine = sqlalchemy.create_engin(database_url)\n",
    "connection = engine.connect()\n",
    "\n",
    "#get_url (without password showing)\n",
    "def get_url():\n",
    "    \"\"\"\n",
    "    verify connection to database\n",
    "    return database connection in URL format:\n",
    "\n",
    "    dialect://user:password@host/dbname\n",
    "\n",
    "    password will be hidden with ***\n",
    "    \"\"\"\n",
    "    url_without_password = repr(connection.engin.url)\n",
    "    return {'database_url': url_without_password}\n",
    "\n",
    "\n",
    "# call function to verify connection to DB\n",
    "get_url()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data/tables using write function\n",
    "def write_data(df):\n",
    "    tablename = 'mytable'\n",
    "    df.to_sql(tablename, connection, if_exists='append', index=False, method='multi'\n",
    "\n",
    "\n",
    "# call function to create pandas df\n",
    "write_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data/tables using read function\n",
    "def read_data(tablename):\n",
    "  \"\"\"\n",
    "  select all unique records from the database\n",
    "  read them into a dataframe\n",
    "  \"\"\"\n",
    "  query = f\"\"\"SELECT DISTINCT * FROM {tablename} LIMIT 5\"\"\"\n",
    "  df = pd.read_sql(query, connection)\n",
    "  return df.to_dict(orient='records')\n",
    "\n",
    "\n",
    "# call function to read string fields from RDB table(s)\n",
    "read_data('pdfs')\n",
    "read_data('judges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "\n",
    "\n",
    "1.   Case folding - the process of changing all letters to lowercase\n",
    "2.   tokenizing - the process of reducing a string to its individual; decrease of documents (parsing) into single words (tokens)\n",
    "1.    Filtering - the process of determining important words from its token\n",
    "2.   Stemming or lemmatiation - technique for reducing word to root word; based on stem or form\n",
    "\n",
    "The purpose of preprocessing data  in the form of numerical values is that this data then becomes the source of data to be processed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def createTokens(df):\n",
    "    '''\n",
    "    function to tokenize, lemmatixe lowercase\n",
    "    and remove stop words from text\n",
    "    '''\n",
    "    tokens = []\n",
    "    for doc in tqdm(nlp.pipe(df.astype('unicode').values),total=df.size):\n",
    "        if doc.is_parsed:\n",
    "            tokens.append([n.lemma_.lower() for n in doc if (no n.is_punct and not n._isspace and not n._is stop)])\n",
    "        else:\n",
    "            tokens.append('')\n",
    "    return tokens\n",
    "\n",
    "## dependent on scraping tool separating documents into csv file type as currently written\n",
    "raw = pd.read_csv('')\n",
    "  # as the tabular query has not been built yet so we have no filename to insert here\n",
    "tokens = createTokens(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dependent on previous code blocks\n",
    "# find relevant keywords\n",
    "text = ''.join(raw.tolist())\n",
    "nlp.max_length = len(text)\n",
    "\n",
    "keywords = []\n",
    "for tokenlist in tqdm(question_tokens):\n",
    "    doc = nlp(''.join(tokenlist))\n",
    "    # extract and rank\n",
    "    extract = textacy.ke.sgrank(doc, ngrams=(1), window_size=2, normalize=None, topn=2, include_pos=['NOUN', 'PROPN'])\n",
    "        for a, b in extract:\n",
    "            keywords.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort unique keywords by frequency\n",
    "resorted = sorted(set(keywords), key=lambda x: keywords.count(x), reverse=True)\n",
    "# most freq unique keywords assigned to bins\n",
    "top20 = resorted[:20]\n",
    "top200 = resorted[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# extract filenames\n",
    "    # assume huge dataset for scalability\n",
    "    # remove root folder\n",
    "    # traverse through index.html to find pattern to remove title\n",
    "\n",
    "\n",
    "# read all the index.html files at once\n",
    "x[0] for x in os.walk(str(os.getcwd()) + '/<foldername>/')]\n",
    "# remove the extra / for the root folder\n",
    "folders[0] = folders[0][:len(folders[0])-1]\n",
    "\n",
    "\n",
    "# use re to match pattern of names and titles\n",
    "names = re.findall(' ', text)\n",
    "  # insert pattern  inside quotes once we have access to files\n",
    "court = re.findall(' ', text)\n",
    "  # insert pattern inside quotes once we have acces to files\n",
    "\n",
    "\n",
    "# iterate\n",
    "# read the file from index filenames\n",
    "data = []\n",
    "\n",
    "for i in folders:\n",
    "  file = open(i + '/index.html', 'r')\n",
    "  text = file.read().strip()\n",
    "  file.close()\n",
    "\n",
    "  # extract title and names\n",
    "  file_name = names\n",
    "  file_court = court \n",
    "\n",
    "  # iterate to next folder\n",
    "  for j in range(len(file_name)):\n",
    "      data.append((str(i) + str(file_name[j]), file_court[j])) \n",
    "\n",
    "# use a conditional to remove\n",
    "if c == False:\n",
    "  file_name = file_name[2:]\n",
    "  c = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Term Frequency â€“ Inverse Document\n",
    "Frequency (TF-IDF) freature weighting is one of the simplest strongest in the industry.  \n",
    "\n",
    "The Term Frequency (TF) method  weights terms based on the frequency the words appears in  a single document.   The higher the TF value of a word in a document, the higher the effect of that term on the document.\n",
    "\n",
    "The Inverse Document Frequency (IDF) is is a weighting method based on the number of words that appear throughout all the documents in a corpus.\n",
    "\n",
    "TD-IDF, in this use case, is a preprocessing dependency for Cosine similariity and clustering thatsupport efficiencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# court - holds description of court instance (ij(initial hearing), bia(appeal))\n",
    "# body\n",
    "# TF-IDF(doc) = (TF-IDF(court) * alpha) +\n",
    "              # (TF-IDF(body) * (1-alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcutate DF\n",
    "DF = {}\n",
    "\n",
    "# iterate through all the documents storing all the document id's for each word\n",
    "# processed_text is the body of the document\n",
    "for i in range(len(processed_text)):\n",
    "  tokens = processed_text[i]\n",
    "  for word in tokens:\n",
    "    try:\n",
    "        # add to the set since the set exists\n",
    "        DF[word].add(i)\n",
    "    except:\n",
    "        # create a set since the word doesn't have a set yet\n",
    "        DF[word] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(DF)->unique words\n",
    "# count of total unique words in vocabulary\n",
    "for i in DF:\n",
    "  DF[i] = len(DF[i])\n",
    "print(DF)\n",
    "\n",
    "\n",
    "# keys of DF\n",
    "total_vocab = [x for x in DF]\n",
    "print(total_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF-IDF\n",
    "tf_idf = {}\n",
    "for i in range(N):\n",
    "    # calculate TF-IDF for body of all docs\n",
    "    tokens_tf_idf = processed_text[i] \n",
    "\n",
    "    # calculate TF-IDF for title of all docs\n",
    "    counter = Counter(tokens + processed_court[i]) \n",
    "\n",
    "    # iterate body TF-IDF for every (doc, token) if  token is in body, \n",
    "    # replace the body(doc, token) value with the value in \n",
    "    # Title(doc, token)\n",
    "    for token in np.unique(tokens):\n",
    "      df = doc_freq(token)\n",
    "      idf = np.log(N/(df + 1))\n",
    "      # multiplythe body with alpha\n",
    "      tf_idf[doc, token] = tf * idf\n",
    "\n",
    "# TF-IDF = body_TF-IDF * body_weight +\n",
    "          # court_TF-IDF * court_weight\n",
    "\n",
    "           # where body_weight + court_weight = 1\n",
    "\n",
    "# returns tuple(doc, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank using matching score\n",
    "def matchingScore(query):\n",
    "    query_weights = {}\n",
    "    for kye in tf_idf:\n",
    "        if key[1] in tokens:\n",
    "            # key[0]->doc, key[1]->token\n",
    "            query_weights[key[0]] += tf_idf[key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Cosine similarity comppares the inner product of space  between two vectors.  It is measured by the cosine of the angle between two vectors pointing along similar paths.  \n",
    "\n",
    "In this use case, it is used to measure measure document sililarity in text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query and documents  to vectors\n",
    "# use np to store document vectors\n",
    "vect = np.zeros((N, total_vocab_size))\n",
    "\n",
    "for i in tf_idf:\n",
    "  # use list of unique tokens to generate index for each token\n",
    "  ind = total_vocab.index(i[1])\n",
    "  vect[i[0]][ind] = tf_idf[i]\n",
    "# tf, df, idf calculations from query and store in np array\n",
    "Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "counter = Counter(tokens)\n",
    "words_count = len(tokens)\n",
    "\n",
    "query_weights = {}\n",
    "\n",
    "for token in np.unique(tokens):\n",
    "  tf = counter[token]/words_count\n",
    "  df = doc_freq(token)N + 1) / (df + 1))\n",
    "\n",
    "# calculate cosign similarity and rturn maximum k documents\n",
    "CS = np.dot(a, b) / (norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbor (KNN) is a supervised learning algorithm that results in new instance query  which is classified by majority KNN category.  The purpose of the KNN algorithm is to classify a new object (document) based on attributes (keywords) and training samples using minimum distances from the query instance to the training samples to determine the K-nearest neighbors.\n",
    "\n",
    "Traditional KNN text classifiers have several limitations.\n",
    "\n",
    "\n",
    "1.   High calculation complexity to find the KNN samples - all the training samples must be calculated\n",
    "1.   Dependence on training set - the  classifier is generated only with the training samples and does not use any additional data\n",
    "2.   no weight difference between samples - doesn't match actual phenomenon where the samples commonly have uneven distribution\n",
    "\n",
    "By combining KNN with K-means, the expected outcome is a reduction in the complex calculation of training set after determining term weighting as describer of document importance in preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# make document X to be same text feature form as training samples\n",
    "# calculate the similarities between all training samples and document X\n",
    "# choose k samples whaich are larger than N similarities and treat them as a KNN collection\n",
    "# calculate the probability of X belonging toeach category respectively \n",
    "# judge doc X to be the category which has the largest cosign similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using K-means\n",
    "Due to the numbers of calculations taken between samples (test and all of the training samples), traditional KNN has great calculation complexity and can be less efficient with larger datasets.\n",
    "\n",
    "Considering scalibility, combine KNN samples which have largest similarities with clustering technique in order to overcome calculation complexity.\n",
    "\n",
    "The purpose of combining K-means with KNN  is to reduce the time for calculating similarities in the KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# calculate weight of each document - sum all term weights and divide by total terms in each document\n",
    "# cluster each large KNN sample using K-means\n",
    "    # initialize the value of K as the number of clusters of doc to be created\n",
    "    # generate the centroid randomly\n",
    "    # assign each object to the group that is closets to centroid\n",
    "    # update centroid by calculating average value of cluster\n",
    "    # repeat until centroids no longer move (convergence)\n",
    "# after clustering for each category, the cluster centers now represent  the new training sets for KNN algorithm \n",
    "# - reducing time needed for calclating similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore LDA as an alternative to K-Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# create confusion matrix - cluster by system (Y/N), cluster is actually (Y/N)\n",
    "# calculate recall\n",
    "# calculate precision\n",
    "# calculate F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar chart\n",
    "specific judge's (historical) grants vs denials of similarly classified cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line graph(s)\n",
    "(historical) term frequency associated with specific judge and denials\n",
    "(including appeallate data)\n",
    "(excluding appeallate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bubble chart\n",
    "comparison of all current seated judges (historical) grants vs denials of similarly classified cases\n",
    "(or alternative bubble chart to be determined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build linear regression model using predictors\n",
    "predictors = []\n",
    "    # update predictors when schema for database is complete\n",
    "    \n",
    "# Split data into predictors X and output Y\n",
    "X = advert[predictors]\n",
    "y = advert['adjuication']\n",
    "\n",
    "# Initialize and fit model\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(X, y)\n",
    "\n",
    "# identify alpha and beta\n",
    "print(f'alpha = {model.intercept_}')\n",
    "print(f'betas = {model.coef_}')\n",
    "\n",
    "# predict judicial outcome\n",
    "model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new predictions\n",
    "new_X = [[_,_]]\n",
    "print(model.predict(new_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--when adding endpoints to ml.py:\n",
    "\n",
    "\n",
    "*   consider null values \n",
    "*   client error/404\n",
    "*   check instance type\n",
    "*   bound integers\n",
    "*   explicitly identify variable object type and allow for conversion\n",
    "*   ***use class method and define function to create json request body***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}