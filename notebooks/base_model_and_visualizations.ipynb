{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: until we have data documents to process, this will serve only as an option for future teams - inherited DB from Labs29 is empty with no schema\n",
    "\n",
    "[Labs 29  HRF Asylum B DS AWS RDS PostgresSQL](https://master:***@asylum.catpmmwmrkhp.us-east-1.rds.amazonaws.com/asylum)\n",
    "\n",
    "[Labs 29 AWS Asylum A DS RDS PostgresSQL](https://master:***rds_endpoint=hrfasylum-database-a.catpmmwmrkhp.us-east-1.rds.amazonaws.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model and Exploratory Visualizations \n",
    "classify the documents (judges decisions) as grant or deny, classify the doduments owner(judge), process text in documents for classification and anlysis of text - glean insights to the granting or the denial, process using combined KNN classification and k-means clusting model, evaluate model, explore data through visualization and statistical analysis methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database dependency\n",
    "!pip install sqlalchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# dataframe tools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# natural language processor\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "# wrapper to show progress/time as self checking tool\n",
    "from tqdm import tqd\n",
    "\n",
    "\n",
    "# key word extraction\n",
    "import textacy\n",
    "import textacy.ke\n",
    "\n",
    "\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "\n",
    "# linear algebra and array/matrices tools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# database tools\n",
    "import sqlalchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database URL\n",
    "database_url = input()\n",
    "\n",
    "#connect to database\n",
    "engine = sqlalchemy.create_engin(database_url)\n",
    "connection = engine.connect()\n",
    "\n",
    "#get_url (without password showing)\n",
    "def get_url():\n",
    "    \"\"\"\n",
    "    verify connection to database\n",
    "    return database connection in URL format:\n",
    "\n",
    "    dialect://user:password@host/dbname\n",
    "\n",
    "    password will be hidden with ***\n",
    "    \"\"\"\n",
    "    url_without_password = repr(connection.engin.url)\n",
    "    return {'database_url': url_without_password}\n",
    "\n",
    "\n",
    "# call function to verify connection to DB\n",
    "get_url()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data/tables using write function\n",
    "def write_data(df):\n",
    "    tablename = 'mytable'\n",
    "    df.to_sql(tablename, connection, if_exists='append', index=False, method='multi'\n",
    "\n",
    "\n",
    "# call function to create pandas df\n",
    "write_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data/tables using read function\n",
    "def read_data(tablename):\n",
    "  \"\"\"\n",
    "  select all unique records from the database\n",
    "  read them into a dataframe\n",
    "  \"\"\"\n",
    "  query = f\"\"\"SELECT DISTINCT * FROM {tablename} LIMIT 5\"\"\"\n",
    "  df = pd.read_sql(query, connection)\n",
    "  return df.to_dict(orient='records')\n",
    "\n",
    "\n",
    "# call function to read string fields from RDB table(s)\n",
    "read_data('pdfs')\n",
    "read_data('judges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "lowercase,\n",
    "parsing,\n",
    "tokenizing,\n",
    "filter for relevance/importance,\n",
    "stemming/lemmatiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def createTokens(df):\n",
    "    '''\n",
    "    function to tokenize, lemmatixe lowercase\n",
    "    and remove stop words from text\n",
    "    '''\n",
    "    tokens = []\n",
    "    for doc in tqdm(nlp.pipe(df.astype('unicode').values),total=df.size):\n",
    "        if doc.is_parsed:\n",
    "            tokens.append([n.lemma_.lower() for n in doc if (no n.is_punct and not n._isspace and not n._is stop)])\n",
    "        else:\n",
    "            tokens.append('')\n",
    "    return tokens\n",
    "\n",
    "## dependent on scraping tool separating documents into csv file type as currently written\n",
    "raw = pd.read_csv('')\n",
    "  # as the tabular query has not been built yet so we have no filename to insert here\n",
    "tokens = createTokens(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dependent on previous code blocks\n",
    "# find relevant keywords\n",
    "text = ''.join(raw.tolist())\n",
    "nlp.max_length = len(text)\n",
    "\n",
    "keywords = []\n",
    "for tokenlist in tqdm(question_tokens):\n",
    "    doc = nlp(''.join(tokenlist))\n",
    "    # extract and rank\n",
    "    extract = textacy.ke.sgrank(doc, ngrams=(1), window_size=2, normalize=None, topn=2, include_pos=['NOUN', 'PROPN'])\n",
    "        for a, b in extract:\n",
    "            keywords.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dependent on previous code blocks\n",
    "# sort unique keywords by frequency\n",
    "resorted = sorted(set(keywords), key=lambda x: keywords.count(x), reverse=True)\n",
    "# top 20 most freq unique keywords\n",
    "top20 = resorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dependent on previous code blocks - description of flow\n",
    "##dependent on previous code blocks - description of flow\n",
    "# extract filenames\n",
    "    # assume huge dataset for scalability\n",
    "    # remove root folder\n",
    "    # traverse through index.html to find pattern to remove title\n",
    "\n",
    "\n",
    "# read all the index.html files at once\n",
    "x[0] for x in os.walk(str(os.getcwd()) + '/<foldername>/')]\n",
    "# remove the extra / for the root folder\n",
    "folders[0] = folders[0][:len(folders[0])-1]\n",
    "\n",
    "\n",
    "# use re to match pattern of names and titles\n",
    "names = re.findall(' ', text)\n",
    "  # insert pattern  inside quotes once we have access to files\n",
    "court = re.findall(' ', text)\n",
    "  # insert pattern inside quotes once we have acces to files\n",
    "\n",
    "\n",
    "# iterate\n",
    "# read the file from index filenames\n",
    "data = []\n",
    "\n",
    "for i in folders:\n",
    "  file = open(i + '/index.html', 'r')\n",
    "  text = file.read().strip()\n",
    "  file.close()\n",
    "\n",
    "  # extract title and names\n",
    "  file_name = names\n",
    "  file_court = court \n",
    "\n",
    "  # iterate to next folder\n",
    "  for j in range(len(file_name)):\n",
    "      data.append((str(i) + str(file_name[j]), file_court[j])) \n",
    "\n",
    "# use a conditional to remove\n",
    "if c == False:\n",
    "  file_name = file_name[2:]\n",
    "  c = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting Terms\n",
    "term frequency (freq the words appears in doc),\n",
    "inverse document frequency (num words that appead in doc),\n",
    "TF-IDF feature weighting - one of the and simplest and strongest in industry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# court - holds description of court instance (ij(initial hearing), bia(appeal))\n",
    "# body\n",
    "# TF-IDF(doc) = (TF-IDF(court) * alpha) +\n",
    "              # (TF-IDF(body) * (1-alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcutate DF\n",
    "DF = {}\n",
    "\n",
    "# iterate through all the documents storing all the document id's for each word\n",
    "# processed_text is the body of the document\n",
    "for i in range(len(processed_text)):\n",
    "  tokens = processed_text[i]\n",
    "  for word in tokens:\n",
    "    try:\n",
    "        # add to the set since the set exists\n",
    "        DF[word].add(i)\n",
    "    except:\n",
    "        # create a set since the word doesn't have a set yet\n",
    "        DF[word] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(DF)->unique words\n",
    "# count of total unique words in vocabulary\n",
    "for i in DF:\n",
    "  DF[i] = len(DF[i])\n",
    "print(DF)\n",
    "\n",
    "\n",
    "# keys of DF\n",
    "total_vocab = [x for x in DF]\n",
    "print(total_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF-IDF\n",
    "tf_idf = {}\n",
    "for i in range(N):\n",
    "    # calculate TF-IDF for body of all docs\n",
    "    tokens_tf_idf = processed_text[i] \n",
    "\n",
    "    # calculate TF-IDF for title of all docs\n",
    "    counter = Counter(tokens + processed_court[i]) \n",
    "\n",
    "    # iterate body TF-IDF for every (doc, token) if  token is in body, \n",
    "    # replace the body(doc, token) value with the value in \n",
    "    # Title(doc, token)\n",
    "    for token in np.unique(tokens):\n",
    "      df = doc_freq(token)\n",
    "      idf = np.log(N/(df + 1))\n",
    "      # multiplythe body with alpha\n",
    "      tf_idf[doc, token] = tf * idf\n",
    "\n",
    "# TF-IDF = body_TF-IDF * body_weight +\n",
    "          # court_TF-IDF * court_weight\n",
    "\n",
    "           # where body_weight + court_weight = 1\n",
    "\n",
    "# returns tuple(doc, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank using matching score\n",
    "def matchingScore(query):\n",
    "    query_weights = {}\n",
    "    for kye in tf_idf:\n",
    "        if key[1] in tokens:\n",
    "            # key[0]->doc, key[1]->token\n",
    "            query_weights[key[0]] += tf_idf[key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "vectorization using cosine similarity for scalability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query and documents  to vectors\n",
    "# use np to store document vectors\n",
    "vect = np.zeros((N, total_vocab_size))\n",
    "\n",
    "for i in tf_idf:\n",
    "  # use list of unique tokens to generate index for each token\n",
    "  ind = total_vocab.index(i[1])\n",
    "  vect[i[0]][ind] = tf_idf[i]\n",
    "# tf, df, idf calculations from query and store in np array\n",
    "Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "counter = Counter(tokens)\n",
    "words_count = len(tokens)\n",
    "\n",
    "query_weights = {}\n",
    "\n",
    "for token in np.unique(tokens):\n",
    "  tf = counter[token]/words_count\n",
    "  df = doc_freq(token)N + 1) / (df + 1))\n",
    "\n",
    "# calculate cosign similarity and rturn maximum k documents\n",
    "CS = np.dot(a, b) / (norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using KNN\n",
    "supervised learning resulting in new instance query classified by majority K-nearest neighbor category,\n",
    "classifies new object (document) based on attributes (words) and training samples using minimum distances from the query instance to the training samples to determine the K-nearest neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# make document X to be same text feature form as training samples\n",
    "# calculate the similarities between all training samples and document X\n",
    "# choose k samples whaich are larger than N similarities and treat them as a KNN collection\n",
    "# calculate the probability of X belonging toeach category respectively \n",
    "# judge doc X to be the category which has the largest cosign similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using K-means\n",
    "combine KNN samples which have largest similarities with clustering technique in order to overcome calculation complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# calculate weight of each document - sum all term weights and divide by total terms in each document\n",
    "# cluster each large KNN sample using K-means\n",
    "    # initialize the value of K as the number of clusters of doc to be created\n",
    "    # generate the centroid randomly\n",
    "    # assign each object to the group that is closets to centroid\n",
    "    # update centroid by calculating average value of cluster\n",
    "    # repeat until centroids no longer move (convergence)\n",
    "# after clustering for each category, the cluster centers now represent  the new training sets for KNN algorithm \n",
    "# - reducing time needed for calclating similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore LDA as an alternative to K-Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## description of flow\n",
    "# create confusion matrix - cluster by system (Y/N), cluster is actually (Y/N)\n",
    "# calculate recall\n",
    "# calculate precision\n",
    "# calculate F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar chart\n",
    "specific judge's (historical) grants vs denials of similarly classified cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line graph(s)\n",
    "(historical) term frequency associated with specific judge and denials\n",
    "(including appeallate data)\n",
    "(excluding appeallate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bubble chart\n",
    "comparison of all current seated judges (historical) grants vs denials of similarly classified cases\n",
    "(or alternative bubble chart to be determined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}