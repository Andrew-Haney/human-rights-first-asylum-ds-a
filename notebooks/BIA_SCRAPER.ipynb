{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIA_SCRAPER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FibDZlR2iQD8"
      },
      "source": [
        "# BOARD OF IMMIGRATION APPEALS DOCUMENT SCRAPER\r\n",
        "\r\n",
        "• This scraper uses the spaCy library in order to gather information about the case. There is no way to ensure it is 100% accurate, except to scrape a document and compare the information gathered directly with the original PDF. \r\n",
        "\r\n",
        "• This code currently lives inside a Jupyter Notebook for ease of testing and iteration, but will ultimately need to graduate into a standard .py file. \r\n",
        "\r\n",
        "• Scraping is extremely difficult task, as in most cases just searching the document for a keyword isn't enough. It must live in the correct context. No douct the code in this notebook can constantly be improved for a very long time. Before delving into this code, I recommend reading a good chunk of case file PDFs in order to get aquianted with their structre and information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEsm4q0_S7mn"
      },
      "source": [
        "%%capture\r\n",
        "!pip install spacy\r\n",
        "!pip install bs4\r\n",
        "!pip install geonamescache"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m21I3dxQSwhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b46f34-ffb2-4cbe-8b1a-673bc5744d22"
      },
      "source": [
        "from typing import List, Tuple, Union, Callable, Dict, Iterator\r\n",
        "from collections import defaultdict\r\n",
        "from pprint import pprint\r\n",
        "from difflib import SequenceMatcher\r\n",
        "from datetime import datetime\r\n",
        "import bs4\r\n",
        "from bs4 import BeautifulSoup, element\r\n",
        "import geonamescache\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "from pathlib import Path\r\n",
        "import re\r\n",
        "import os\r\n",
        "\r\n",
        "import spacy\r\n",
        "from spacy.tokens.doc import Doc\r\n",
        "from spacy.tokens.span import Span\r\n",
        "from spacy.tokens.token import Token\r\n",
        "spacy.cli.download(\"en_core_web_lg\")\r\n",
        "\r\n",
        "# I currently interface with the text files through my google drive.\r\n",
        "# The relevant documents should be able upon request through Slack,\r\n",
        "# and ideally should at some point be stored and obtained through a DB\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2cTaSDPb9yK"
      },
      "source": [
        "ASYLUM_DIR: Path\r\n",
        "ASYLUM_DIR = Path(\"./drive/MyDrive/asylum_cases_txt\")\r\n",
        "\r\n",
        "text_files: List[str]\r\n",
        "text_files = os.listdir(ASYLUM_DIR)\r\n",
        "\r\n",
        "def get_text_from(text_file: str, dir: str = ASYLUM_DIR):\r\n",
        "    with open(dir / text_file) as f:\r\n",
        "        text = f.read()\r\n",
        "    return text\r\n",
        "\r\n",
        "ex_text = get_text_from(text_files[1])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6vRaYDB0Mib"
      },
      "source": [
        "# loading in the spaCy model\r\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ian7APwuirJU"
      },
      "source": [
        "# Extracting current Appellate Immigration Judges\r\n",
        "# From Wikipedia, using Beautiful Soup. Code is mostly biolerplate\r\n",
        "judges_url: str\r\n",
        "judges_url = 'https://en.wikipedia.org/wiki/Board_of_Immigration_Appeals'\r\n",
        "\r\n",
        "html: str\r\n",
        "html = requests.get(judges_url).text\r\n",
        "\r\n",
        "soup: BeautifulSoup\r\n",
        "soup = BeautifulSoup(html, 'html.parser')\r\n",
        "\r\n",
        "table: element.Tag\r\n",
        "table = soup.findAll(lambda tag: tag.name =='table')[1]\r\n",
        "\r\n",
        "rows: element.ResultSet\r\n",
        "rows = table.findAll(lambda tag: tag.name == 'tr')\r\n",
        "\r\n",
        "column_names: List[str]\r\n",
        "column_names = [\r\n",
        "    col.get_text().strip().lower().replace(' ', '_')\r\n",
        "    for col in rows.pop(0) if col.name == 'th'\r\n",
        "]\r\n",
        "\r\n",
        "rows: List[List[str]]\r\n",
        "rows = [\r\n",
        "    [\r\n",
        "        cell.get_text().strip().replace(',', '') \r\n",
        "        for cell in row if cell.name == 'td'\r\n",
        "    ] \r\n",
        "    for row in rows\r\n",
        "]\r\n",
        "\r\n",
        "as_dict: List[Dict[str, str]]\r\n",
        "as_dict = [\r\n",
        "    dict(zip(column_names, row)) for row in rows\r\n",
        "]\r\n",
        "\r\n",
        "judges_df: pd.DataFrame\r\n",
        "judges_df = pd.DataFrame.from_dict(as_dict)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeN1jtnRkBgs"
      },
      "source": [
        "• The Optical Character Recognition (OCR) used through pytesseract to convert PDFs to text ISN'T perfect. Often artifacts show up, such as the character 'l' shown up as '!', or 'nn' as 'm'. \r\n",
        "\r\n",
        "• The difflib library comes in handy here in order to compare strings by inexact values. The more two strings are alike, the sequence matcher ratio will approach 1, the less they are alike, it will approach 0.\r\n",
        "\r\n",
        "• I recently found a library that could offer a better implementation. It's called FuzzyWuzzy, future teams should look into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bepD85LnBm9x"
      },
      "source": [
        "# I would like to later reimplement these with the python\r\n",
        "# library FuzzyWuzzy, look it up!\r\n",
        "def similar(a: str, return_b: str, min_score: float) -> str:\r\n",
        "    '''\r\n",
        "    • Returns 2nd string if similarity score is above supplied\r\n",
        "    minimum score. Else, returns None.\r\n",
        "    '''\r\n",
        "    return return_b \\\r\n",
        "        if SequenceMatcher(None, a, return_b).ratio() >= min_score \\\r\n",
        "        else None\r\n",
        "\r\n",
        "# this function implements the similar function, but on a list\r\n",
        "# it uses a closure in order to return a function\r\n",
        "def similar_in_list(lst: List[str]) -> Callable:\r\n",
        "    '''\r\n",
        "    • Uses a closure on supplied list to return a function that iterates over\r\n",
        "    the list in order to search for the first similar term. It's used widely\r\n",
        "    in the scraper.\r\n",
        "    '''\r\n",
        "    def impl(item: str, min_score: float) -> Union[str, None]:\r\n",
        "        for s in lst:\r\n",
        "            s = similar(item, s, min_score)\r\n",
        "            if s:\r\n",
        "                return s\r\n",
        "        return None\r\n",
        "    return impl"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m052dd5O_Dvi"
      },
      "source": [
        "def get_if_judge(name: str) -> Union[str, None]:\r\n",
        "    '''\r\n",
        "    • Returns the judge's name if a match is found. Currently, the match\r\n",
        "    is very strictly defined by the current judge's names found through\r\n",
        "    Wikipedia. It will 100% stop any false positives, but some leniency should\r\n",
        "    be introduced in order to prevent any false negatives.\r\n",
        "    '''\r\n",
        "    clean_name: Callable[[str], str]\r\n",
        "    clean_name = lambda s: s.lower() \\\r\n",
        "                            .replace(',', '') \\\r\n",
        "                            .replace('.', '')\r\n",
        "\r\n",
        "    # Tuple of split, sorted judge name, and original judge name\r\n",
        "    judges_names: List[Tuple[List[str], str]]\r\n",
        "    judges_names = [\r\n",
        "        (sorted(clean_name(jn).split()), jn)\r\n",
        "        for jn in judges_df['name']\r\n",
        "    ]   \r\n",
        "    \r\n",
        "    name: List[str]\r\n",
        "    name = sorted(map(clean_name, name.split()))\r\n",
        "\r\n",
        "    for jn_low, jn in judges_names:\r\n",
        "        is_judge: bool\r\n",
        "        is_judge = all([\r\n",
        "            similar(n, j, 0.8)\r\n",
        "            for n, j in zip(name, jn_low)\r\n",
        "            if len(n) != 1\r\n",
        "        ])\r\n",
        "\r\n",
        "        if is_judge:\r\n",
        "            return jn\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG0fYt4vGgxh"
      },
      "source": [
        "# Use geonamescache library in order to get current list of all countries\r\n",
        "gc = geonamescache.GeonamesCache()\r\n",
        "\r\n",
        "COUNTRIES: Iterator[str]\r\n",
        "COUNTRIES = gc.get_countries_by_names().keys()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AXvG3fFeJaz"
      },
      "source": [
        "class BIACase:\r\n",
        "    def __init__(self, text: str):\r\n",
        "        '''\r\n",
        "        • Input will be text from a BIA case pdf file, after the pdf has\r\n",
        "        been converted from PDF to text. \r\n",
        "        • Scraping works utilizing spaCy, tokenizing the text, and iterating\r\n",
        "        token by token searching for matching keywords.\r\n",
        "        '''\r\n",
        "\r\n",
        "        self.doc: Doc \r\n",
        "        self.doc = nlp(text)\r\n",
        "\r\n",
        "        self.ents: Tuple[Span] \r\n",
        "        self.ents = self.doc.ents\r\n",
        "\r\n",
        "    def get_ents(self, labels: List[str] = None) -> Iterator[Span]:\r\n",
        "        '''\r\n",
        "        • Retrieves entitiess of a specified label(s) in the document,\r\n",
        "        if no label is specified, returns all entities\r\n",
        "        '''\r\n",
        "        return (ent for ent in self.ents if ent.label_ in labels) \\\r\n",
        "                if labels \\\r\n",
        "                else self.ents\r\n",
        "\r\n",
        "    def get_country_of_origin(self) -> str:\r\n",
        "        '''\r\n",
        "        • Returns the country of origin of the applicant. Currently just checks\r\n",
        "        the document for a country that is NOT the United States.\r\n",
        "        '''\r\n",
        "        locations: Iterator[str]\r\n",
        "        locations = map(lambda ent: ent.text,\r\n",
        "                        self.get_ents(['GPE']))\r\n",
        "        \r\n",
        "        similar_country: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_country = similar_in_list(COUNTRIES)\r\n",
        "\r\n",
        "        for loc in locations:\r\n",
        "            if not similar(loc, 'United States', 0.9):\r\n",
        "                origin: Union[str, None]\r\n",
        "                origin = similar_country(loc, 0.9)\r\n",
        "\r\n",
        "                if origin: return origin\r\n",
        "                else: continue\r\n",
        "        \r\n",
        "        return None\r\n",
        "\r\n",
        "\r\n",
        "    def get_date(self) -> Union[str, None]:\r\n",
        "        '''\r\n",
        "        • Returns date of the document. Easy to validate by the PDF filename,\r\n",
        "        whether its hosted on scribd or somewhere else.\r\n",
        "        '''\r\n",
        "        clean_date: Callable[[str], str]\r\n",
        "        clean_date = lambda s: ''.join([\r\n",
        "            char for char in s\r\n",
        "                if char.isalnum()\r\n",
        "                or char.isspace()\r\n",
        "        ])\r\n",
        "\r\n",
        "        dates: Iterator[str] \r\n",
        "        dates = map(lambda ent: clean_date(ent.text), \r\n",
        "                    self.get_ents(['DATE']))\r\n",
        "\r\n",
        "        for date in dates:\r\n",
        "            try:\r\n",
        "                # SHOULD return list of length 3,\r\n",
        "                # Such as ['Sept', '2', '2019']\r\n",
        "                d: List[str]\r\n",
        "                d = date.split()\r\n",
        "                if len(d) != 3:\r\n",
        "                    continue\r\n",
        "                else:\r\n",
        "                    # Ex. Jan, Feb, ..., Sep, Oct, Dec\r\n",
        "                    month: str\r\n",
        "                    month = d[0][:3].title()\r\n",
        "                    # Ex. 01, 02, 03, ..., 29, 30, 31\r\n",
        "                    day: str\r\n",
        "                    day = '0' + d[1] \\\r\n",
        "                        if len(d[1]) == 1 else d[1]\r\n",
        "                    # Ex. 1991, 1992, ..., 2020, 2021\r\n",
        "                    year: str\r\n",
        "                    year = d[2]\r\n",
        "                    # Ex. Jan 09 2014\r\n",
        "                    parsed_date: str\r\n",
        "                    parsed_date = ' '.join([month, day, year])\r\n",
        "                    # datetime obj, Ex Repr: 2020-09-24 00:00:00\r\n",
        "                    dt: datetime\r\n",
        "                    dt = datetime.strptime(parsed_date, '%b %d %Y')\r\n",
        "                    # strip time of hours/min/sec, save as str\r\n",
        "                    dt: str\r\n",
        "                    dt = str(dt).split()[0]\r\n",
        "                    \r\n",
        "                    return dt\r\n",
        "            except:\r\n",
        "                continue\r\n",
        "\r\n",
        "        return None\r\n",
        "\r\n",
        "    def get_panel(self) -> Union[List[str], None]:\r\n",
        "        '''\r\n",
        "        • Returns the panel members of case in document. \r\n",
        "        TODO: Check judges names less strictly - I've seen a document\r\n",
        "        that named the Judge Monsky differently than how she regularly \r\n",
        "        appears.\r\n",
        "        '''\r\n",
        "        panel_members = List[str]\r\n",
        "        panel_members = []\r\n",
        "\r\n",
        "        possible_members: Iterator[Span]\r\n",
        "        possible_members = map(lambda ent: ent.text,\r\n",
        "                               self.get_ents(['PERSON', 'ORG']))\r\n",
        "\r\n",
        "        for member in possible_members:\r\n",
        "            judge: Union[str, None]\r\n",
        "            judge = get_if_judge(member)\r\n",
        "\r\n",
        "            if judge:\r\n",
        "                panel_members.append(judge)\r\n",
        "\r\n",
        "        return list(set(panel_members)) \\\r\n",
        "               if panel_members \\\r\n",
        "               else None\r\n",
        "\r\n",
        "    def get_surrounding_sents(self, token: Token) -> Span:\r\n",
        "        '''\r\n",
        "        • This function will return the two sentences surrounding the token,\r\n",
        "        including the sentence holding the token.\r\n",
        "        '''\r\n",
        "        start: int\r\n",
        "        start = token.sent.start\r\n",
        "\r\n",
        "        end: int\r\n",
        "        end = token.sent.end\r\n",
        "\r\n",
        "        try:\r\n",
        "            sent_before_start: int\r\n",
        "            sent_before_start = self.doc[start-1].sent.start\r\n",
        "            sent_after_end: int\r\n",
        "            sent_after_end = self.doc[end+1].sent.end\r\n",
        "        except:\r\n",
        "            return token.sent\r\n",
        "\r\n",
        "        surrounding: Span\r\n",
        "        surrounding = self.doc[sent_before_start:sent_after_end+1]\r\n",
        "\r\n",
        "        return surrounding\r\n",
        "\r\n",
        "    def get_protected_grounds(self) -> Union[List[str], None]:\r\n",
        "        '''\r\n",
        "        • This will return the protected ground(s) of the applicant. Special\r\n",
        "        checks are needed. Checking for keywords is not enough, as sometimes\r\n",
        "        documents label laws that describe each protected ground. Examples\r\n",
        "        are 'Purely Political Offense' and 'Real Id Act'.\r\n",
        "        '''\r\n",
        "        protected_grounds: List[str]\r\n",
        "        protected_grounds = [\r\n",
        "            'race',\r\n",
        "            'religion',\r\n",
        "            'nationality',\r\n",
        "            'social',\r\n",
        "            'political',\r\n",
        "        ]\r\n",
        "\r\n",
        "        pgs = []\r\n",
        "\r\n",
        "        similar_pg: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_pg = similar_in_list(protected_grounds)\r\n",
        "\r\n",
        "        for token in self.doc:\r\n",
        "\r\n",
        "            sent: str\r\n",
        "            sent = token.sent.text.lower()\r\n",
        "\r\n",
        "            s: Union[str, None]\r\n",
        "            s = similar_pg(token.text.lower(), 0.9)\r\n",
        "\r\n",
        "            if s == 'social':\r\n",
        "                next_word = self.doc[token.i+1].text.lower()\r\n",
        "                if not similar(next_word, 'group', 0.95): \r\n",
        "                    continue\r\n",
        "\r\n",
        "            elif s == 'political':\r\n",
        "                next_word = self.doc[token.i+1].text.lower()\r\n",
        "                if similar(next_word, 'offense', 0.95): \r\n",
        "                    continue\r\n",
        "\r\n",
        "            elif s == 'nationality':\r\n",
        "                next_word = self.doc[token.i+1].text.lower()\r\n",
        "                if similar(next_word, 'act', 1):\r\n",
        "                    continue\r\n",
        "\r\n",
        "            if s:\r\n",
        "                surrounding: Span\r\n",
        "                surrounding = self.get_surrounding_sents(token)\r\n",
        "                \r\n",
        "                if 'real id' in sent:\r\n",
        "                    continue\r\n",
        "                elif 'grounds specified' in surrounding.text.lower():\r\n",
        "                    continue \r\n",
        "                elif 'no claim' in surrounding.text.lower():\r\n",
        "                    continue\r\n",
        "\r\n",
        "                pgs.append(s)\r\n",
        "\r\n",
        "        return list(set(pgs)) if pgs else None\r\n",
        "\r\n",
        "    def get_application(self) -> Dict[str, bool]:\r\n",
        "        '''\r\n",
        "        • This will return the seeker's application, found after 'APPLICATION'.\r\n",
        "        Because HRF is only interested in Asylum, Withholding of Removal,\r\n",
        "        and Convention Against Torture applications, the others should be\r\n",
        "        ignored and not included in the dataset.\r\n",
        "        '''\r\n",
        "\r\n",
        "        relevant_applications: List[str]\r\n",
        "        relevant_applications = [\r\n",
        "            'asylum',\r\n",
        "            'withholding',\r\n",
        "            'torture'    \r\n",
        "        ]\r\n",
        "\r\n",
        "        similar_app: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_app = similar_in_list(relevant_applications)\r\n",
        "\r\n",
        "        app: Dict[str, bool]\r\n",
        "        application = {\r\n",
        "            'asylum': False,\r\n",
        "            'withholding_of_removal': False,\r\n",
        "            'CAT': False\r\n",
        "        }\r\n",
        "\r\n",
        "        for token in self.doc:\r\n",
        "            if similar(token.text, 'APPLICATION', .86):\r\n",
        "                for i in range(1,30):\r\n",
        "                    word: str\r\n",
        "                    word = self.doc[i + token.i].text.lower()\r\n",
        "\r\n",
        "                    app: Union[str, None]\r\n",
        "                    app = similar_app(word, 0.9)\r\n",
        "\r\n",
        "                    if app == 'asylum':\r\n",
        "                        application['asylum'] = True\r\n",
        "                    elif app == 'withholding':\r\n",
        "                        application['withholding_of_removal'] = True\r\n",
        "                    elif app == 'torture':\r\n",
        "                        application['CAT'] = True\r\n",
        "\r\n",
        "        return application\r\n",
        "\r\n",
        "    def get_outcome(self) -> Union[str, None]:\r\n",
        "        '''\r\n",
        "        • Returns the outcome of the case. This will appear after 'ORDER'\r\n",
        "        at the end of the document.\r\n",
        "        '''\r\n",
        "        outcomes: List[str]\r\n",
        "        outcomes = [\r\n",
        "            'remanded', \r\n",
        "            'reversal', \r\n",
        "            'dismissed', \r\n",
        "            'sustained', \r\n",
        "            'terminated', \r\n",
        "            'granted', \r\n",
        "            'denied', \r\n",
        "            'returned'\r\n",
        "        ]\r\n",
        "\r\n",
        "        outcomes: Iterator[str]\r\n",
        "        outcomes_lemma = map(lambda s: nlp(s)[0].lemma_, outcomes)\r\n",
        "\r\n",
        "        similar_outcome: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_outcome = similar_in_list(outcomes)\r\n",
        "\r\n",
        "        similar_outcome_l: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_outcome_l = similar_in_list(outcomes)\r\n",
        "\r\n",
        "        dlen: int\r\n",
        "        dlen = len(self.doc)\r\n",
        "\r\n",
        "        # iterating token by token through document in reverse\r\n",
        "        # improves efficiency only slightly\r\n",
        "        for i in reversed(range(dlen-1)):\r\n",
        "            token: Token\r\n",
        "            token = self.doc[i]\r\n",
        "\r\n",
        "            if similar(token.text, 'ORDER', 0.9):\r\n",
        "                for ii in range(i+1, dlen):\r\n",
        "                    o: Union[str, None]\r\n",
        "                    o = similar_outcome(self.doc[ii].text, 0.9)\r\n",
        "                    o = o if o else similar_outcome_l(self.doc[ii].text, 0.92)\r\n",
        "                    if o:\r\n",
        "                        return nlp(o)[0].lemma_\r\n",
        "        return None\r\n",
        "\r\n",
        "    def get_based_violence(self) -> Union[Dict[str, List[str]], None]:\r\n",
        "        '''\r\n",
        "        • Returns a dictionary where the keys are:\r\n",
        "            Family-based violence,\r\n",
        "            Gender-based violence,\r\n",
        "            Gang-based violence\r\n",
        "        • If a key is in the dict, it means the based_violence is present\r\n",
        "        in the document, and the relevant sentence(s) where the information is\r\n",
        "        contained in the key's value\r\n",
        "        '''\r\n",
        "        violent_terms: List[str]\r\n",
        "        violent_terms = [\r\n",
        "            'hurt',\r\n",
        "            'kill',\r\n",
        "            'rape',\r\n",
        "            'assassinate',\r\n",
        "            'abuse',\r\n",
        "            'threat',\r\n",
        "            'murder',\r\n",
        "            'torture',\r\n",
        "            'assault',\r\n",
        "            'shoot',\r\n",
        "            'suffer',\r\n",
        "            'abduct',\r\n",
        "            'kidnap',\r\n",
        "            'harm',\r\n",
        "            'persecute'\r\n",
        "        ]\r\n",
        "\r\n",
        "        sg_family: List[str]\r\n",
        "        sg_family = [\r\n",
        "            'family',\r\n",
        "            'woman',\r\n",
        "            'partner',\r\n",
        "            'husband',\r\n",
        "            'wife',\r\n",
        "            'son',\r\n",
        "            'daughter',\r\n",
        "            'child',\r\n",
        "            'ethnicity'\r\n",
        "        ]\r\n",
        "\r\n",
        "        sg_gender: List[str]\r\n",
        "        sg_gender = [\r\n",
        "            'sex'\r\n",
        "            'gender',\r\n",
        "            'sexuality',\r\n",
        "            'woman',\r\n",
        "            'transgender',\r\n",
        "            'lgbt',\r\n",
        "            'lgbtq'\r\n",
        "            'homosexual',\r\n",
        "            'homosexuality',\r\n",
        "            'gay',\r\n",
        "            'lesbian',\r\n",
        "            'queer',\r\n",
        "        ]\r\n",
        "\r\n",
        "        similar_vterm: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_vterm = similar_in_list(violent_terms)\r\n",
        "\r\n",
        "        similar_sg_family: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_sg_family = similar_in_list(sg_family)\r\n",
        "\r\n",
        "        similar_sg_gender: Callable[[str, float], Union[str, None]]\r\n",
        "        similar_sg_gender = similar_in_list(sg_gender)\r\n",
        "\r\n",
        "        based_v = defaultdict(lambda: [])\r\n",
        "\r\n",
        "        for token in self.doc:\r\n",
        "            if similar_sg_family(token.text.lower(), 0.86):\r\n",
        "                sent: Span\r\n",
        "                sent = token.sent\r\n",
        "                for w in sent:\r\n",
        "                    vterm = similar_vterm(w.lemma_.lower(), 0.86)\r\n",
        "                    if vterm and 'statute' not in token.sent.text:\r\n",
        "                        based_v['family-based'] += [token.sent.text]\r\n",
        "\r\n",
        "            elif similar_sg_gender(token.text.lower(), 0.86):\r\n",
        "                sent: Span\r\n",
        "                sent = self.get_surrounding_sents(token)\r\n",
        "                for w in sent:\r\n",
        "                    vterm = similar_vterm(w.lemma_.lower(), 0.86)\r\n",
        "                    if vterm and 'statute' not in token.sent.text:\r\n",
        "                        based_v['gender-based'] += [token.sent.text]\r\n",
        "            \r\n",
        "            elif similar(token.text.lower(), 'gang', 0.9):\r\n",
        "                sent = token.sent\r\n",
        "                based_v['gang-based'] += [sent.text]\r\n",
        "\r\n",
        "        if based_v:\r\n",
        "            based_v: Dict[str, List[str]]\r\n",
        "            based_v = {k:list(set(v)) for k, v in based_v.items()}\r\n",
        "\r\n",
        "        return based_v if based_v else None\r\n",
        "\r\n",
        "    def references_AB27_216(self) -> bool:\r\n",
        "        '''\r\n",
        "        • Returns True if the case file mentions \r\n",
        "        Matter of AB, 27 I&N Dec. 316 (A.G. 2018)\r\n",
        "        '''\r\n",
        "        for token in self.doc:\r\n",
        "            if token.text == 'I&N':\r\n",
        "                sent = token.sent.text\r\n",
        "                if '316' in sent and '27' in sent:\r\n",
        "                    return True\r\n",
        "        return False\r\n",
        "\r\n",
        "    def references_LEA27_581(self) -> bool:\r\n",
        "        '''\r\n",
        "        • Returns True if the case file mentions \r\n",
        "        Matter of L-E-A-, 27 I&N Dec. 581 (A.G. 2019)\r\n",
        "        '''\r\n",
        "        for sent in self.doc.sents:\r\n",
        "            if 'L-E-A-' in sent.text:\r\n",
        "                if '27' in sent.text:\r\n",
        "                    return True\r\n",
        "        return False\r\n",
        "                \r\n",
        "    def get_seeker_sex(self) -> str:\r\n",
        "        '''\r\n",
        "        • This field needs to be validated. Currently, it assumes the \r\n",
        "        sex of the seeker by the number of instances of pronouns in the \r\n",
        "        document.\r\n",
        "        '''\r\n",
        "        male: int\r\n",
        "        male = 0\r\n",
        "\r\n",
        "        female: int\r\n",
        "        female = 0\r\n",
        "\r\n",
        "        for token in self.doc:\r\n",
        "            if similar(token.text, 'he', 1) \\\r\n",
        "                or similar(token.text, 'him', 1) \\\r\n",
        "                or similar(token.text, 'his', 1):\r\n",
        "                male += 1\r\n",
        "            elif similar(token.text, 'she', 1) \\\r\n",
        "                or similar(token.text, 'her', 1):\r\n",
        "                female += 1\r\n",
        "\r\n",
        "        return 'male' if male > female \\\r\n",
        "                else 'female' if female > male \\\r\n",
        "                else 'unkown'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN87zvLWi1rB"
      },
      "source": [
        "# data = []\r\n",
        "\r\n",
        "# for f in text_files:\r\n",
        "\r\n",
        "#     case = BIACase(get_text_from(f))\r\n",
        "#     case_data = {}\r\n",
        "\r\n",
        "#     case_data['filename'] = f[:-4]\r\n",
        "#     case_data['application'] = case.get_application()\r\n",
        "#     case_data['date'] = case.get_date()\r\n",
        "#     case_data['country_of_origin'] = case.get_country_of_origin()\r\n",
        "#     case_data['panel_members'] = case.get_panel()\r\n",
        "#     case_data['outcome'] = case.get_outcome()\r\n",
        "#     case_data['protected_grounds'] = case.get_protected_grounds()\r\n",
        "#     case_data['based_violence'] = case.get_based_violence()\r\n",
        "#     case_data['AB_27_I&N_216'] = case.references_AB27_216()\r\n",
        "#     case_data['L-E-A_27_I&N_581'] = case.references_LEA27_581()\r\n",
        "#     case_data['sex_of_asylum_seeker'] = case.get_seeker_sex()\r\n",
        "\r\n",
        "#     pprint(case_data)\r\n",
        "#     data.append(case_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dccBfiXlpbx",
        "outputId": "a5021e12-811c-4551-c4b2-99aa81eb8193"
      },
      "source": [
        "print('spaCy',spacy.__version__)\r\n",
        "print('bs4', bs4.__version__)\r\n",
        "print('geonamescache', geonamescache.__version__)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy 2.2.4\n",
            "bs4 4.6.3\n",
            "geonamescache 1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}